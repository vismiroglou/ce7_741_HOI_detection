{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS WHERE I TEST STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from src.dataset import CropsDataset, ObjectDetectorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from src.utils import collate_fn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(['human-ride-bicycle', 'human-walk-bicycle', 'human-hold-bicycle', 'human-ride-motorcycle', 'human-walk-motorcycle'])\n",
    "dataset = CropsDataset(anno_file=r'../annotations_hoi_frame.csv', img_dir = r'../Image Dataset', label_encoder=le)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate through dataloader\n",
    "#image, target, coord = next(iter(dataloader))\n",
    "#plt.imshow(image[0].numpy().transpose(1, 2 , 0))\n",
    "#print(le.inverse_transform(target[0].numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "data_list = []\n",
    "data_list2 = []\n",
    "for batch in dataloader:\n",
    "    crop, label, coord = batch\n",
    "    data_list.append((coord,label))\n",
    "    data_list2.append((crop,label))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training clf on bbox coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_list[533][1])\n",
    "print(len(data_list))\n",
    "print(le.inverse_transform(np.ravel(data_list[0][1])))\n",
    "\n",
    "# Separate features and labels in separate lists\n",
    "# Reshaping labels cause classifier expect dim <= 2\n",
    "features, labels = zip(*data_list)\n",
    "labels = np.array(labels).reshape(-1)\n",
    "features = np.array([coord[0] for coord in features])\n",
    "print(labels.shape)\n",
    "#print(features)\n",
    "print(features.shape)\n",
    "#features = [image[0].reshape(-1).numpy() for image in features]\n",
    "\n",
    "# Split the data into training and testing sets with random seed\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Creating a RandomForestClassifier with 100 trees with random seed\n",
    "# Trying to fit the features (cropped frames of human+object)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=seed)\n",
    "rf_classifier.fit(features,labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\"\"\"\n",
    "### Observation ###\n",
    "Due to different shapes of the cropped frames\n",
    "i cant train the classifier on the features.\n",
    "\n",
    "I might try padding the crops?\n",
    "\n",
    "Trying to pass coordinates instead, works.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training clf on cropped imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1], dtype=torch.int32),)\n",
      "535\n",
      "['human-ride-motorcycle']\n",
      "(210,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\simon\\OneDrive\\Dokumenter\\ComTek7-github\\ce7_741_HOI_detection\\temp copy.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#print(features[0][0].shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#for img in features:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#    print(img[0].shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Find the maximum height and width among all crops\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m max_height \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(img\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m img \u001b[39min\u001b[39;49;00m features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m max_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(img\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(ch,\u001b[39m\u001b[39m{\u001b[39;00mmax_height\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mmax_width\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\simon\\OneDrive\\Dokumenter\\ComTek7-github\\ce7_741_HOI_detection\\temp copy.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(features[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#print(features[0][0].shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#for img in features:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#    print(img[0].shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Find the maximum height and width among all crops\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m max_height \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(img\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m max_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(img\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/OneDrive/Dokumenter/ComTek7-github/ce7_741_HOI_detection/temp%20copy.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(ch,\u001b[39m\u001b[39m{\u001b[39;00mmax_height\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mmax_width\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "print(data_list2[533][1])\n",
    "print(len(data_list2))\n",
    "print(le.inverse_transform(np.ravel(data_list2[0][1])))\n",
    "\n",
    "# Separate features and labels in separate lists\n",
    "# Reshaping labels cause classifier expect dim <= 2\n",
    "features, labels = zip(*data_list2)\n",
    "labels = np.array(labels).reshape(-1)\n",
    "#features = np.array([coord[0] for coord in features])\n",
    "#print(labels.shape)\n",
    "#print(features)\n",
    "#print(features.shape)\n",
    "features = [image[0].numpy().reshape(-1) for image in features]\n",
    "#features = [image[0].numpy() for image in features]\n",
    "print(features[0].shape)\n",
    "\n",
    "\n",
    "#print(features[0][0].shape)\n",
    "#for img in features:\n",
    "#    print(img[0].shape)\n",
    "\n",
    "# Find the maximum height and width among all crops\n",
    "max_height = max(img.shape[1] for img in features)\n",
    "max_width = max(img.shape[2] for img in features)\n",
    "\n",
    "print(f\"(ch,{max_height},{max_width})\")\n",
    "test = np.empty((3, max_height, max_width))\n",
    "print(\"test\",test.shape)\n",
    "\n",
    "# Pad each crop to match the maximum height and width\n",
    "features = [np.pad(img,pad_width=1) for img in features]\n",
    "\n",
    "# Split the data into training and testing sets with random seed\n",
    "seed = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Creating a RandomForestClassifier with 100 trees with random seed\n",
    "# Trying to fit the features (cropped frames of human+object)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=seed)\n",
    "rf_classifier.fit(features,labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
